# Desafio de Engenharia de Dados: Pipeline ETL para o Banco BanVic

## Introdu√ß√£o

Este projeto √© a minha solu√ß√£o para o desafio de engenharia de dados proposto, que simula a cria√ß√£o de um pipeline de extra√ß√£o, transforma√ß√£o e carga (ETL) para o banco fict√≠cio BanVic. O objetivo √© extrair dados de m√∫ltiplas fontes (um banco de dados SQL e um arquivo CSV), centraliz√°-los e carreg√°-los em um Data Warehouse para futuras an√°lises.

Todo o ambiente foi desenvolvido utilizando Docker e Docker Compose para garantir a **reprodutibilidade**, um requisito fundamental do desafio.

## Arquitetura da Solu√ß√£o

O fluxo de dados segue a arquitetura abaixo, onde o Apache Airflow orquestra todo o processo de extra√ß√£o dos dados das fontes, o armazenamento tempor√°rio em um FileSystem local e o carregamento final no Data Warehouse em PostgreSQL.

```mermaid
graph TD;
    subgraph "Fontes de Dados"
        A(fa:fa-file-csv CSV);
        B(fa:fa-database SQL - PostgreSQL);
    end

    subgraph "Pipeline Orquestrado por Apache Airflow"
        C{Extra√ß√£o};
        D[FileSystem Local<br><i>(√Årea de Stage)</i>];
        E{Carregamento};
        F(fa:fa-warehouse Data Warehouse<br><i>PostgreSQL</i>);
    end

    A -- "transacoes.csv" --> C;
    B -- "Tabelas do ERP" --> C;
    C -- "Dados extra√≠dos em CSV" --> D;
    D -- "Arquivos di√°rios" --> E;
    E -- "Carga idempotente" --> F;
```

## üõ†Ô∏è Tecnologias Utilizadas

* **Orquestra√ß√£o:** Apache Airflow
* **Containeriza√ß√£o:** Docker & Docker Compose
* **Bancos de Dados:** PostgreSQL
* **Linguagem:** Python
* **Bibliotecas Principais:** Pandas (para manipula√ß√£o de dados), Psycopg2, SQLAlchemy

## üöÄ Como Executar o Projeto

Para executar este projeto em sua m√°quina local, siga os passos abaixo.

### Pr√©-requisitos

* [Git](https://git-scm.com/)
* [Docker Desktop](https://www.docker.com/products/docker-desktop/)

### Passos para a Execu√ß√£o

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/jampani1/airflow-docker-etl]
    cd [EG_desafio]
    ```

2.  **Configure as Vari√°veis de Ambiente:**
    Crie um arquivo chamado `.env` na raiz do projeto, copiando o conte√∫do do arquivo 

    Conte√∫do do seu `.env` deve ser:
    ```
    AIRFLOW_UID=50000

    POSTGRES_AIRFLOW_USER=airflow
    POSTGRES_AIRFLOW_PASSWORD=airflow
    POSTGRES_AIRFLOW_DB=airflow_db

    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow_db:5432/airflow_db
    AIRFLOW__WEBSERVER__ALLOW_CONN_TEST=True
    ```

3.  **Suba o Ambiente Docker:**
    Execute o seguinte comando. Na primeira vez, ele ir√° construir a imagem customizada do Airflow, o que pode levar alguns minutos.
    ```bash
    docker-compose up -d --build
    ```

4.  **Acesse a Interface do Airflow:**
    Ap√≥s a conclus√£o do comando, aguarde cerca de 1 a 2 minutos para que os servi√ßos iniciem. Depois, acesse a interface no seu navegador:
    * **URL:** `http://localhost:8080`
    * **Usu√°rio:** `admin`
    * **Senha:** `admin`

5.  **Execute a DAG:**
    * Na interface, encontre a DAG `banvic_pipeline`.
    * Ative-a no bot√£o √† esquerda.
    * Clique no √≠cone de "Play" (‚ñ∂Ô∏è) √† direita e selecione "Trigger DAG" para iniciar a execu√ß√£o.

6.  **Verifique os Resultados:**
    Ap√≥s a DAG rodar com sucesso, voc√™ pode verificar os dados carregados no Data Warehouse:
    ```bash
    docker-compose exec banvic_db psql -U data_engineer -d banvic
    ```
    Dentro do psql, execute:
    ```sql
    \dt dw.*;
    SELECT * FROM dw.clientes LIMIT 5;
    ```

## üìà Estrutura do Pipeline (DAG)

A DAG `banvic_pipeline` foi implementada utilizando a TaskFlow API do Airflow e segue a l√≥gica abaixo:

![Grafo da DAG](image_05b51e.png)

* **Extra√ß√£o em Paralelo:** As tarefas `extrair_csv` e `extrair_sql` rodam simultaneamente para otimizar o tempo de execu√ß√£o.
* **Carregamento Condicional:** A tarefa `carregar_dw` s√≥ √© iniciada ap√≥s a conclus√£o bem-sucedida de **ambas** as tarefas de extra√ß√£o, garantindo a consist√™ncia dos dados.
* **Idempot√™ncia:** A tarefa de carregamento utiliza a estrat√©gia `if_exists='replace'`, o que torna o pipeline idempotente. Cada execu√ß√£o apaga os dados antigos e insere a nova carga completa.

## üß† Desafios e Aprendizados

A constru√ß√£o deste projeto foi uma jornada pr√°tica de aprendizado, especialmente na configura√ß√£o do ambiente. Os principais desafios superados foram:

* **Conflitos de Vers√£o:** O principal obst√°culo foi diagnosticar e resolver um conflito de vers√£o entre a biblioteca `pandas` e a vers√£o do `Python` contida na imagem Docker oficial do Airflow. A solu√ß√£o envolveu um processo de depura√ß√£o dos logs de `build` para identificar a incompatibilidade e ajustar o arquivo `requirements.txt` para usar vers√µes que funcionassem em harmonia.
* **Configura√ß√£o do Dockerfile:** A imagem do Airflow possui regras de seguran√ßa espec√≠ficas para a instala√ß√£o de pacotes. Foi um desafio encontrar a combina√ß√£o correta de `Dockerfile`, vers√£o da imagem base (`2.8.4-LTS`) e m√©todo de instala√ß√£o (`pip install --user`) que resultasse em uma imagem customizada est√°vel e funcional.
* **Conectividade entre Cont√™ineres:** Foi necess√°rio configurar uma rede Docker para permitir que os servi√ßos do Airflow se comunicassem com o cont√™iner do banco de dados `banvic_db` usando os nomes dos servi√ßos como `hostname`.

-------- 

## Desenvolvido por

üßë‚Äçüíª Este projeto foi desenvolvido por mim, Maur√≠cio J Souza, como uma demonstra√ß√£o de habilidades em Engenharia de Dados (ED) e Docker!

Para considera√ß√µes, perguntas ou oportunidades, sinta-se √† vontade para entrar em contato:

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/mauriciojampani/)
[![Gmail](https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:mmjampani13@gmail.com)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/jampani1)